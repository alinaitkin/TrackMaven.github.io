<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Maven">
    <link rel="icon" href="https://s3.amazonaws.com/awstrackmaven/img/favicon.ico">
    <link rel="alternate" type="application/rss+xml" title="RSS" href="http://engineroom.trackmaven.com/feeds/rss.xml">

    <title>
3-2-1 Backup of Postgres on AWS to S3 and offsite server - The Engine Room - TrackMaven
    </title>
    <script type="text/javascript">
      (function(d) {
        var config = {
          kitId: 'mgb7pvk',
          scriptTimeout: 3000
        },
        h=d.documentElement,t=setTimeout(function(){h.className=h.className.replace(/\bwf-loading\b/g,"")+" wf-inactive";},config.scriptTimeout),tk=d.createElement("script"),f=false,s=d.getElementsByTagName("script")[0],a;h.className+=" wf-loading";tk.src='//use.typekit.net/'+config.kitId+'.js';tk.async=true;tk.onload=tk.onreadystatechange=function(){a=this.readyState;if(f||a&&a!="complete"&&a!="loaded")return;f=true;clearTimeout(t);try{Typekit.load(config)}catch(e){}};s.parentNode.insertBefore(tk,s)
      })(document);
    </script>
    <script src="//use.typekit.net/mgb7pvk.js"></script>
    <script>try{Typekit.load();}catch(e){}</script>

    <link href="http://engineroom.trackmaven.com/theme/css/github.css" rel="stylesheet">
    <link href="http://engineroom.trackmaven.com/theme/css/main.css" rel="stylesheet">
  </head>
  <body>
    <div class="wrap">
      <div class="blog-masthead">
        <div class="container">
          <div class="row">
            <div class="col-md-6">
              <a href="http://engineroom.trackmaven.com/"><img class="logo" src="http://engineroom.trackmaven.com/theme/images/logo@2x.png"/ ></a>
              <h1 class="blog-title">
                <a href="http://engineroom.trackmaven.com/">The Engine Room</a>
              </h1>
            </div>
            <div class="col-md-6 mobile-placement">
              <a class="ribbon float-right" href="https://boards.greenhouse.io/trackmaven/jobs/41377">We're Hiring!</a>
            </div>
          </div>
        </div>
      </div>
       <div class="container">
<div class="blog-post single">
  <h2 class="blog-post-title centered">3-2-1 Backup of Postgres on AWS to S3 and offsite server</h2>
  <p class="blog-post-meta single">Posted on <strong>November 09, 2014</strong> by <a href="http://engineroom.trackmaven.com/author/fred-battista/">Fred Battista</a><br>Category: <a href="/category/backups/">Backups</a> | <a href="http://engineroom.trackmaven.com/blog/3-2-1-backup-of-postgres-on-aws-to-s3-and-offsite-server/#disqus_thread"></a></p>
  <div class="blog-post-content">
    <blockquote>
<p>3 copies, 2 different media types, 1 offsite, boom, good to go.</p>
<p>--<cite> Abraham Lincoln</cite></p>
</blockquote>
<p>Backups should be straightforward, automatic, have few moving parts, adhere to the <a href="http://www.dpbestflow.org/backup/backup-overview#321">3-2-1 rule</a>, and, most importantly, facilitate easy recovery.</p>
<p>TrackMaven's applications run on AWS and we've always stored several backups on S3. However, best practices suggest thaht you keep a backup offsite if it is feasible to do so.</p>
<p>This post details one way to keep multiple backup copies of your database both in S3 and on an offsite server.</p>
<h2>Preparing your database machine for backup</h2>
<h3>Create an additional mountpoint for your backup data</h3>
<p>Unless you have a significant amount of extra space on your DB machines we suggest creating and mounting an additional EBS volume to handle your backup date.</p>
<p>The instructions to <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-volume.html">create</a>, <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-volume.html">attach</a>, and <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-attaching-volume.html">mount</a> another EBS volume (hereafter referrered to as <code>/YOUR_BACKUP_POINT</code>) are covered by Amazon in the provided links.</p>
<h3>Setting permissions on new volume</h3>
<p>Because you created the mountpoint with <code>sudo</code> it will be owned by <code>root</code>; we need it to be readable/writeable by other users. Which other users should have access?</p>
<p><code>pg_dump</code> is the process we will use to write the backup, and it should be run by the <code>postgres</code> user so it does not have to authenticate database access (annoying and difficult to do securely within <code>crontab</code>).</p>
<p>We created a new group <code>BACKUPUSERS</code> and added our <code>ssh_user</code> and <code>postgres</code> to it with these commands:</p>
<div class="highlight"><pre>sudo groupadd BACKUPUSERS
sudo usermod -a -G BACKUPUSERS YOUR_SSH_USER
sudo usermod -a -G BACKUPUSERS postgres
sudo chgrp -R BACKUPUSERS /YOUR_BACKUP_POINT
sudo chmod -R 770 /YOUR_BACKUP_POINT
</pre></div>


<p>H/T to <a href="http://superuser.com/questions/280994/give-write-permissions-to-multiple-users-on-a-folder-in-ubuntu">this superuser answer</a>.</p>
<h2>Initial backup</h2>
<p>If you haven't created an IAMS user with only S3 permissions, we suggest that you do that now because you'll need the keys for the next step.</p>
<p>Give the backup S3 user both read and write permissions if you feel your local machine is secure enough to warrant it.</p>
<p>Install and configure the S3 command line client on the DB machine:</p>
<div class="highlight"><pre>sudo apt-get install s3cmd
s3cmd --configure
</pre></div>


<p>Put the following script somewhere on your actual DB machine - uncomment the logging lines if you are running manually and want to debug the script behavior.</p>
<p>This script will remove yesterday's backups, dump a current copy of the database, compress the current copy of the database, and ship it to S3.</p>
<h3>DB backup cron script</h3>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46</pre></div></td><td class="code"><div class="highlight"><pre><span class="c">#!/bin/bash</span>

<span class="c"># Backup script to pg_dump</span>
<span class="c"># &#39;YOUR_DATABASE&#39; db. Assumes that</span>
<span class="c"># this is on the crontab of the</span>
<span class="c"># postgres user so no authentication</span>
<span class="c"># is necessary.</span>


<span class="nv">PGDUMP</span><span class="o">=</span>/usr/bin/pg_dump
<span class="nv">DATABASE</span><span class="o">=</span>YOUR_DATABASE
<span class="nv">BACKUP_FOLDER</span><span class="o">=</span>/YOUR_BACKUP_FOLDER
<span class="nv">EXPORTFILE</span><span class="o">=</span><span class="nv">$BACKUP_FOLDER</span>/pg_dump_<span class="sb">`</span>date +%F<span class="sb">`</span>.sql
<span class="nv">COMPRESSEDFILE</span><span class="o">=</span><span class="nv">$EXPORTFILE</span>.tgz
<span class="nv">BUCKET</span><span class="o">=</span>s3://YOUR_S3_BUCKET/OPTIONAL_FOLDER/
<span class="nv">S3CMD</span><span class="o">=</span>/usr/bin/s3cmd
<span class="nv">LOG_FILE</span><span class="o">=</span><span class="nv">$BACKUP_FOLDER</span>/backup_log_file.txt
<span class="nv">DATE</span><span class="o">=</span><span class="sb">`</span>date +%F<span class="sb">`</span>

<span class="nv">REMOVE_TIME</span><span class="o">=</span><span class="sb">`</span>date +%T<span class="sb">`</span>
<span class="nb">echo </span>Time: <span class="nv">$REMOVE_TIME</span>
<span class="nb">echo </span>Removing yesterdays...
<span class="nb">echo</span> <span class="nv">$DATE</span>,RemovingOld,<span class="nv">$REMOVE_TIME</span> &gt;&gt; <span class="nv">$LOG_FILE</span>
rm <span class="nv">$BACKUP_FOLDER</span>/*.sql*

<span class="nv">DUMP_TIME</span><span class="o">=</span><span class="sb">`</span>date +%T<span class="sb">`</span>
<span class="nb">echo </span>Time: <span class="nv">$DUMP_TIME</span>
<span class="nb">echo </span>Dumping...
<span class="nb">echo</span> <span class="nv">$DATE</span>,DumpBegan,<span class="nv">$DUMP_TIME</span> &gt;&gt; <span class="nv">$LOG_FILE</span>
<span class="nv">$PGDUMP</span> -c -f <span class="nv">$EXPORTFILE</span> <span class="nv">$DATABASE</span>

<span class="nv">TAR_TIME</span><span class="o">=</span><span class="sb">`</span>date +%T<span class="sb">`</span>
<span class="nb">echo </span>Time: <span class="nv">$TAR_TIME</span>
<span class="nb">echo </span>Taring...
<span class="nv">$DATE</span>,TarBegan,<span class="nv">$TAR_TIME</span> &gt;&gt; <span class="nv">$LOG_FILE</span>
tar -czf <span class="nv">$COMPRESSEDFILE</span> <span class="nv">$EXPORTFILE</span>

<span class="nv">S3_TIME</span><span class="o">=</span><span class="sb">`</span>date +%T<span class="sb">`</span>
<span class="nb">echo </span>Time: <span class="nv">$S3_TIME</span>
<span class="nb">echo </span>s3cmd PUTTING...
<span class="nb">echo</span> <span class="nv">$DATE</span>,S3Began,<span class="nv">$S3_TIME</span> &gt;&gt; <span class="nv">$LOG_FILE</span>
<span class="nv">$S3CMD</span> put <span class="nv">$COMPRESSEDFILE</span> <span class="nv">$BUCKET</span>
<span class="nv">$S3CMD</span> put -f <span class="nv">$LOG_FILE</span> <span class="nv">$BUCKET</span>

<span class="nv">DONE_TIME</span><span class="o">=</span><span class="sb">`</span>date +%T<span class="sb">`</span>
<span class="nb">echo</span> <span class="nv">$DATE</span>,Done,<span class="nv">$DONE_TIME</span> &gt;&gt; <span class="nv">$LOG_FILE</span>
</pre></div>
</td></tr></table>

<h3>Set up the crontab</h3>
<p>Become the <code>postgres</code> user so you don't have to authenticate in your crontab to access the database:</p>
<p><code>su - postgres</code></p>
<p>Access your crontab to edit:</p>
<p><code>crontab -e</code></p>
<p>Then add this line to your crontab, which will run your script at the path you specify at 1AM every night:</p>
<p><code>`0 1 * * * /YOUR_BACKUP_POINT/YOUR_BACKUP_SCRIPT.sh</code></p>
<h2>Pull the backup to remote server</h2>
<p>Install <code>s3cmd</code> on your local/offsite box, configure it, and create a backup directory structure if you're game.</p>
<h3>Get/ifnotthenwait/tryagain</h3>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50</pre></div></td><td class="code"><div class="highlight"><pre><span class="c">#!/bin/bash</span>

<span class="c"># Script to test if today&#39;s backup HAS</span>
<span class="c"># occured, and has been pushed to the</span>
<span class="c"># specified S3 bucket - if it has, gets</span>
<span class="c"># the file. If not present, wait 30 min</span>
<span class="c"># and try again.</span>
<span class="c"># Assumptions/Notes:</span>
<span class="c">#  * Only checks 10 times after</span>
<span class="c">#    initial run</span>
<span class="c">#  * Assumes that s3cmd has been</span>
<span class="c">#    configured prior to cron init</span>


<span class="nv">BUCKET</span><span class="o">=</span>s3://YOUR_BUCKET
<span class="nv">DATE</span><span class="o">=</span><span class="sb">`</span>date +%F<span class="sb">`</span>
<span class="nv">FILE</span><span class="o">=</span>pg_dump_<span class="nv">$DATE</span><span class="s2">&quot;&quot;</span>.sql.tgz<span class="s2">&quot;&quot;</span>
<span class="nv">FULLPATH</span><span class="o">=</span><span class="nv">$BUCKET$FILE</span>
<span class="nv">EXISTS</span><span class="o">=</span><span class="nb">false</span>
<span class="nv">WAIT_TIME_IN_SECONDS</span><span class="o">=</span>1800
<span class="nv">RETRYS</span><span class="o">=</span>0
<span class="nv">LOCALPATH</span><span class="o">=</span>yourlocalpathhere/<span class="nv">$FILE</span>

<span class="nb">echo </span>Checking <span class="k">for</span> <span class="nv">$FULLPATH</span>

get_file_from_s3 <span class="o">()</span>
<span class="o">{</span>
  <span class="c">#echo getting $FULLPATH</span>
  s3cmd get <span class="nv">$FULLPATH</span> <span class="nv">$LOCALPATH</span>
<span class="o">}</span>

check_if_backup_complete <span class="o">()</span>
<span class="o">{</span>
  s3cmd info <span class="nv">$FULLPATH</span> &gt;/dev/null 2&gt;<span class="p">&amp;</span>1
  <span class="k">if</span> <span class="o">[</span> <span class="nv">$?</span> -eq <span class="m">0</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="nv">EXISTS</span><span class="o">=</span><span class="nb">true</span>
<span class="nb">    </span>get_file_from_s3
  <span class="k">fi</span>
<span class="o">}</span>

<span class="k">while</span> <span class="o">[</span> <span class="nv">$RETRYS</span> -lt <span class="m">10</span> <span class="o">]</span><span class="p">;</span> <span class="k">do</span>
  check_if_backup_complete
  <span class="k">if</span> ! <span class="nv">$EXISTS</span><span class="p">;</span> <span class="k">then</span>
    sleep <span class="nv">$WAIT_TIME_IN_SECONDS</span>
    <span class="o">((</span> RETRYS++ <span class="o">))</span>
    <span class="c"># echo $RETRYS</span>
  <span class="k">else</span>
    <span class="nb">break</span>
<span class="nb">  </span><span class="k">fi</span>
<span class="k">done</span>
</pre></div>
</td></tr></table>

<p>You can run this script as any user which has been authed for S3.</p>
<h2>Keep only 3 backups on S3</h2>
<p>It is important to not let S3 balloon with tons of backups!</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19</pre></div></td><td class="code"><div class="highlight"><pre><span class="c">#!/bin/bash</span>

<span class="nv">FILES</span><span class="o">=()</span>
FILES+<span class="o">=</span><span class="sb">`</span>s3cmd ls s3://trackmaven-prod-db/pg_dump/ <span class="p">|</span> grep tgz <span class="p">|</span> awk <span class="s1">&#39;{print $4}&#39;</span><span class="sb">`</span>

<span class="c">#echo $FILES</span>

<span class="nv">SORTEDFILES</span><span class="o">=(</span> <span class="k">$(</span>
    <span class="k">for</span> el in <span class="s2">&quot;</span><span class="si">${</span><span class="nv">FILES</span><span class="p">[@]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">do</span>
        <span class="nb">echo</span> <span class="s2">&quot;</span><span class="nv">$el</span><span class="s2">&quot;</span>
    <span class="k">done</span> <span class="p">|</span> sort -Vr <span class="k">)</span> <span class="o">)</span>

<span class="k">for</span> <span class="o">((</span> <span class="nv">i</span> <span class="o">=</span> <span class="m">0</span> <span class="p">;</span> i &lt; <span class="si">${#</span><span class="nv">SORTEDFILES</span><span class="p">[@]</span><span class="si">}</span> <span class="p">;</span> i++ <span class="o">))</span> <span class="k">do</span>
  <span class="k">if</span> <span class="o">[</span> <span class="nv">$i</span> -gt <span class="m">2</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="c">#echo Removing ${SORTEDFILES[$i]}</span>
    s3cmd del <span class="si">${</span><span class="nv">SORTEDFILES</span><span class="p">[</span><span class="nv">$i</span><span class="p">]</span><span class="si">}</span>
  <span class="k">fi</span>
<span class="k">done</span>
</pre></div>
</td></tr></table>

<h2>Keep only n backups on the remote machine</h2>
<p>Then you need to delete the files on the local machine that you don't need anymore:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19</pre></div></td><td class="code"><div class="highlight"><pre><span class="c">#!/bin/bash</span>

<span class="nv">FILES</span><span class="o">=()</span>
<span class="nv">FILES</span><span class="o">=(</span> /your_backup_location/* <span class="o">)</span>

<span class="c">#echo $FILES</span>

<span class="nv">SORTEDFILES</span><span class="o">=(</span> <span class="k">$(</span>
    <span class="k">for</span> el in <span class="s2">&quot;</span><span class="si">${</span><span class="nv">FILES</span><span class="p">[@]</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">do</span>
        <span class="nb">echo</span> <span class="s2">&quot;</span><span class="nv">$el</span><span class="s2">&quot;</span>
    <span class="k">done</span> <span class="p">|</span> sort -Vr <span class="k">)</span> <span class="o">)</span>

<span class="k">for</span> <span class="o">((</span> <span class="nv">i</span> <span class="o">=</span> <span class="m">0</span> <span class="p">;</span> i &lt; <span class="si">${#</span><span class="nv">SORTEDFILES</span><span class="p">[@]</span><span class="si">}</span> <span class="p">;</span> i++ <span class="o">))</span> <span class="k">do</span>
  <span class="k">if</span> <span class="o">[</span> <span class="nv">$i</span> -gt <span class="m">2</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="c">#echo Removing ${SORTEDFILES[$i]}</span>
    s3cmd del <span class="si">${</span><span class="nv">SORTEDFILES</span><span class="p">[</span><span class="nv">$i</span><span class="p">]</span><span class="si">}</span>
  <span class="k">fi</span>
<span class="k">done</span>
</pre></div>
</td></tr></table>

<p>Add these scripts to your <code>crontab</code> as well and you are good to go.</p>

<p class="blog-post-meta footer-meta">Tags: <a href="/tag/ops/">ops</a>, <a href="/tag/backups/">backups</a>, <a href="/tag/best-practices/">best-practices</a></p>  </div>
  <div class="well">
    <div class="row">
      <div class="col-md-3">
        <span class="side-image well-image fred-battista-avatar col-sm-3"></span>
      </div>
      <div class="col-md-9">
        <h3>Fred Battista</h3>
<p class="well-content">I am a Software Maven at <a href="http://trackmaven.com/">TrackMaven</a>.</p>
<p class="well-content">Follow me on <a href="https://twitter.com/nowfred">Twitter</a></p>      </div>
    </div>
  </div>
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</div><!-- /.blog-post -->
        </div>
      </div>
    </div>
    <div class="blog-footer">
      <p>&copy; <script>document.write(new Date().getFullYear())</script><noscript>2015</noscript> - TrackMaven, Inc &bull; <a href="/feeds/rss.xml">RSS</a></p>
    </div>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-54925402-1', 'auto');
  ga('send', 'pageview');
</script>
<script type="text/javascript">
var disqus_shortname = 'tmengineroom';
(function() {
  // Comments
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);

  // Comment Count
  var s = document.createElement('script'); s.async = true;
  s.type = 'text/javascript';
  s.src = '//' + disqus_shortname + '.disqus.com/count.js';
  (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>
  </body>
</html>